{"cells":[{"cell_type":"code","source":["!pip install spacy-transformers\n","#modelo para la comparación de palabras\n","!python -m spacy download en_core_web_lg \n","#libreía de extracción pke\n","!pip install git+https://github.com/boudinfl/pke.git\n","!python -m nltk.downloader stopwords\n","!python -m nltk.downloader universal_tagset\n","!python -m spacy download en "],"metadata":{"id":"vnLDyDlU4ABo"},"id":"vnLDyDlU4ABo","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"f73f70de-7b7f-4a3b-bfb8-ad30ee7f0359","metadata":{"id":"f73f70de-7b7f-4a3b-bfb8-ad30ee7f0359"},"outputs":[],"source":["# Frameworks\n","import numpy as np\n","import pandas as pd\n","from google.colab import drive\n","from bs4 import BeautifulSoup as bs\n","import requests\n","import copy\n","import re\n","#importacion de librerias a utilizar\n","import nltk #utilizada para las stopwords\n","nltk.download('punkt')\n","import pke\n","from pke.lang import stopwords\n","import spacy #vectorizacion y similitud de las palabras\n","import en_core_web_sm\n","import spacy_transformers\n","import en_core_web_lg\n","#librerías para la tokenización y el limpiado de los datos\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","from gensim.parsing.preprocessing import STOPWORDS\n","# librerias para autorizar escribir un sheet de drive\n","from google.colab import auth\n","import gspread\n","from google.auth import default"]},{"cell_type":"code","source":["# Initializacion google drive\n","drive.mount('/content/drive')\n","\n","# Inicializacion del modelo \"en_core_web_lg\" de Spacy\n","nlp = en_core_web_lg.load()\n","\n","# Linea para añadir stopwords a la lista que se tiene por default\n","all_stopwords_gensim = STOPWORDS.union(set(['var', 'anticipates','font' ,'datalayer','dataLayer','emoji','domcontentloaded','attachEvent','gtm4wp','wp','gtm','array','flag', 'n', 't','contactemoji','wpemoji','twemoji','important','year','jquery','rev'])) #Añadir palabras a stopwords list default"],"metadata":{"id":"qMG6do48i4O0"},"id":"qMG6do48i4O0","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":4,"id":"86f3d1d5-56c6-471e-865e-4b5a5faf6258","metadata":{"id":"86f3d1d5-56c6-471e-865e-4b5a5faf6258","executionInfo":{"status":"ok","timestamp":1657157159389,"user_tz":300,"elapsed":10,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"outputs":[],"source":["# ------------------------INICIO DE EJECUCION EXTRACCION DE EXPRESIONES REGULARES--------------\n","\n","\n","\n","# ---------------Optional-----------------------------\n","# Add field 'ID_text' for reading all files text saved.\n","# url_len = len(df_main)\n","# list_text = [str(i)+'.txt' for i in range(url_len)]\n","\n","# df_main[\"id_txt\"] = list_text\n","# df_main.head(2)"]},{"cell_type":"code","source":["# Function to read all urls and save the text content as text file format\n","def get_urls(df):\n","    df2 = df[[\"Url\"]]\n","    cont=0\n","    names = []\n","    for url in df2[\"Url\"]:\n","        \n","        r = requests.get(url)\n","        soup = bs(r.text, 'lxml')\n","        \n","        name_txt= str(cont)+'.txt'\n","        names.append(name_txt)\n","        cont = cont + 1\n","        \n","        f = open('/content/drive/MyDrive/Colab Notebooks/file_txt/'+name_txt, \"w\") \n","        f.write(soup.text)\n","        f.close()\n","    \n","    df2['id_txt'] = names       \n","    return df2"],"metadata":{"id":"Za6BfnaR8fjM","executionInfo":{"status":"ok","timestamp":1657157159389,"user_tz":300,"elapsed":8,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"Za6BfnaR8fjM","execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Function to get the reference id by each link\n","def get_opportunity_id(list_id, file):\n","    try:\n","        list_id.append(re.findall('(SFOP[0-9]*|SFOP-[0-9]*|SFOP-[a-zA-Z]*|SFOP[a-zA-Z]*)', file)[0])\n","    except:\n","        list_id.append(\"\")\n","\n","    return list_id"],"metadata":{"id":"hsfson93-IoG","executionInfo":{"status":"ok","timestamp":1657157159960,"user_tz":300,"elapsed":578,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"hsfson93-IoG","execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Function that receives a slice text and compare if corresponding to some month\n","def has_a_month(item):\n","  months = ['january', 'february', 'march','april', 'may', 'june', 'july', 'august', 'september', 'october', 'november', 'december']\n","  for month in months:\n","    if month in item:\n","      return True\n","  return False\n","\n","# Function to clean text and define feature format\n","def extract_date(row):\n","  for row_entry in row:\n","    row_as_list = row_entry.replace(':','\\n').split('\\n')\n","    for item in row_as_list:\n","      if has_a_month(item.lower()):\n","        word=item.lower().strip().replace('\\xa0',' ').replace(\",\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"\\xe2\",\"\").replace(\" at 11\",\"\")\n","        word=re.sub('th ',' ',word)\n","        word=re.sub('59 pm e(s)?(d)?t (on)?',' ',word)\n","        return pd.to_datetime(item.strip(), errors='ignore'),pd.to_datetime(word.strip(), errors='ignore')\n","  return ''"],"metadata":{"id":"uZLWM_caJyyi","executionInfo":{"status":"ok","timestamp":1657157159975,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"uZLWM_caJyyi","execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Function to get the opening date of each announcement\n","def get_date_open(list_date_open, file):    \n","    try:\n","        exp=re.findall('([\\n]Announcement issuance date:.+([\\xa0]|[\\n]))|([\\n]Funding Opportunity Announcement(\\nBureau of Democracy, Human Rights, and Labor\\n)?.+([\\xa0]|[\\n]))',file)[0]\n","        list_date_open.append(extract_date(exp)[0])\n","    except:\n","        list_date_open.append(\"\")\n","\n","    return list_date_open"],"metadata":{"id":"0CYSCt6w8V2I","executionInfo":{"status":"ok","timestamp":1657157159976,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"0CYSCt6w8V2I","execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Function to get the closing date of each announcement\n","def get_date_close(list_date_close, file):\n","    try: \n","        exp=re.findall('([\\n](Proposal )?[Ss]ubmission [Dd]eadline.+([\\xa0]|[\\n]))|([\\n]Application [Dd]eadline.+([\\xa0]|[\\n]))',file)[0]\n","        list_date_close.append(extract_date(exp)[1]) \n","    except:\n","        list_date_close.append(\"\")\n","   \n","    return list_date_close"],"metadata":{"id":"cJlUWDgXK4pU","executionInfo":{"status":"ok","timestamp":1657157159977,"user_tz":300,"elapsed":53,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"cJlUWDgXK4pU","execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Define if contains the sign\n","def has_a_sign(item):\n","  if \"$\" in item:\n","      return True\n","  return False\n","\n","# Create list with slice text that contains some value or amount\n","def extract_amount(row_list):\n","  amts_found = []\n","  for tuple_in_row in row_list:\n","    for string_in_tuple in tuple_in_row:\n","      if(has_a_sign(string_in_tuple)):\n","        string_in_tuple_2=string_in_tuple.split('$')\n","        for elemt in string_in_tuple_2:\n","          amts_found.append(''.join(re.findall('[0-9,]',elemt)))\n","  if amts_found[0]=='' and amts_found[1]!='':\n","    amts_found[0]=amts_found[1]\n","  elif amts_found[0]=='' and amts_found[1]=='' and amts_found[2]!='':\n","    amts_found[0]=amts_found[2]\n","  elif amts_found[0]=='' and amts_found[1]=='' and amts_found[2]==''and amts_found[3]!='':\n","    amts_found[0]=amts_found[3]\n","  else:\n","    amts_found[0]\n","    \n","  return float(amts_found[0].replace(\",\",\"\"))"],"metadata":{"id":"DmhjZnEEPHHz","executionInfo":{"status":"ok","timestamp":1657157159977,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"DmhjZnEEPHHz","execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Function to get the minimium value of announcement\n","def get_amount_min(amount, file):\n","    try:\n","        exp=re.findall('([\\n]Total [Ff]unding [Ff]loor.+[\\n])|([\\n][Ff]unding [Ff]loor.+[\\n])|([\\n][Ff]unding [Ll]imit.+[\\n])',file)\n","        amount.append(extract_amount(exp))\n","    except:\n","        amount.append(\"\")\n","    \n","    return amount"],"metadata":{"id":"vOubr-kPK9qN","executionInfo":{"status":"ok","timestamp":1657157159978,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"vOubr-kPK9qN","execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Function to get the high value of announcement\n","def get_amount_max(amount, file):\n","    try:\n","        exp=re.findall('([\\n][Ff]unding [Cc]eiling.+[\\n])|([\\xa0 ][Ff]unding [Cc]eiling.+[\\n])|([\\n][Tt]otal [Ff]unding [Cc]eiling.+[\\n])|(and be no more than.+[\\n])',file)\n","        amount.append(extract_amount(exp))\n","    except:\n","        amount.append(\"\")\n","    \n","    return amount"],"metadata":{"id":"MaLX8oo2P6e1","executionInfo":{"status":"ok","timestamp":1657157159979,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"MaLX8oo2P6e1","execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Validate if slice text does match with keyword list 'contr'\n","def has_a_cont(item):\n","  contr = ['Open', 'Cooperative', 'concept notes', 'Concept notes']\n","  for month in contr:\n","    if month in item:\n","      return True\n","  return False\n","\n","def extract_contr(row_list):\n","  conts_found = []\n","  for tuple_in_row in row_list:\n","    for string_in_tuple in tuple_in_row:\n","      row_as_list = string_in_tuple.replace(':','\\n').split('\\n')\n","      for item in row_as_list:\n","        if has_a_cont(item):\n","          return item.strip()\n","  return ''"],"metadata":{"id":"ZKwgSdoDQ5Gm","executionInfo":{"status":"ok","timestamp":1657157159979,"user_tz":300,"elapsed":51,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"ZKwgSdoDQ5Gm","execution_count":13,"outputs":[]},{"cell_type":"code","source":["# Function to get announcement contribution type from regular expresion\n","def get_contribution_type_id(contribution, file):\n","\n","    try:\n","        exp=re.findall('([\\n][Aa]nnouncement [tT]ype.+[\\n])|([\\n][Tt]ype of [Ss]olicitation.+[\\n])|([\\n][Cc]oncept [Nn]otes.+[\\n])',file)\n","        contribution.append(extract_contr(exp))\n","    except:\n","        contribution.append(\"\")\n","    \n","    return contribution"],"metadata":{"id":"iJs20JlmADol","executionInfo":{"status":"ok","timestamp":1657157159980,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"execution_count":14,"outputs":[],"id":"iJs20JlmADol"},{"cell_type":"code","source":["# Function to clean text of description\n","def extract_desc(row):\n","  desc_found = []\n","  for rows in row:\n","    row_as_list = rows.replace('\\n','').split('\\n')\n","    return row_as_list[0]\n","  return ''"],"metadata":{"id":"noE7s_MTRdFi","executionInfo":{"status":"ok","timestamp":1657157159981,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"noE7s_MTRdFi","execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Function to get announcement description from regular expresion\n","def get_description(description, file):\n","    try:\n","        exp=re.findall('(([\\n]FY.+[\\n]*)|([\\n]DRL.+[\\n]))',file)[0]\n","        description.append(extract_desc(exp))\n","    except:\n","        description.append(\"\")\n","        \n","    return description"],"metadata":{"id":"9OK1VmrqRd7y","executionInfo":{"status":"ok","timestamp":1657157159981,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"9OK1VmrqRd7y","execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Function extracts the region of each title in the announcement, from a preloaded list of countries\n","def extract_region(list_region_global, file, data_into_list):\n","  region_section = []\n","  list_region = []\n","  region_section.append(re.split('-', file)[0])\n","\n","  for i in data_into_list:\n","    if re.findall(i, region_section[0]):\n","      list_region.append(i)\n","\n","  list_to_string = ', '.join(list_region)\n","  list_region_global.append(list_to_string)\n","  \n","  return list_region_global"],"metadata":{"id":"sR449TBJR_8Y","executionInfo":{"status":"ok","timestamp":1657157159982,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"sR449TBJR_8Y","execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Main function to get all words scrapyed of text and generate dataframe output\n","def main_regex(df):\n","  df2 = df[[\"Url\",\"id_txt\"]]\n","  id_cov = []\n","  date_open =[]\n","  date_close =[]\n","  amount_min =[]\n","  amount_max =[]\n","  contri_type =[]\n","  desc = []\n","  list_region_global = []\n","  \n","  # Read countries data\n","  file_countries = open('/content/drive/MyDrive/Colab Notebooks/all_countries.txt', 'r')\n","  data = file_countries.read()\n","  data_into_list = data.split(\"\\n\")\n","\n","  # Initialization cicle for extract variables with regex\n","  for k in df2[\"id_txt\"]:\n","    file = open('/content/drive/MyDrive/Colab Notebooks/file_txt/'+k, \"r\")\n","    document = file.read()\n","    get_opportunity_id(id_cov, document)\n","    get_date_open(date_open, document)\n","    get_date_close(date_close, document)\n","    get_amount_min(amount_min, document)\n","    get_amount_max(amount_max, document)\n","    get_contribution_type_id(contri_type, document)\n","    get_description(desc, document)\n","    extract_region(list_region_global, document, data_into_list)\n","    file.close()\n","\n","    \n","  df2[\"id_cov\"] = id_cov\n","  df2[\"date_open\"] = date_open\n","  df2[\"date_close\"] = date_close\n","  df2[\"amount_min\"] = amount_min\n","  df2[\"amount_max\"] = amount_max\n","  df2[\"contribution_type\"] = contri_type\n","  df2[\"description\"] = desc\n","  df2[\"region\"] = list_region_global\n","\n","  return df2"],"metadata":{"id":"NWbVCdxBSGGp","executionInfo":{"status":"ok","timestamp":1657157159982,"user_tz":300,"elapsed":51,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"NWbVCdxBSGGp","execution_count":18,"outputs":[]},{"cell_type":"code","source":["# --------------------INICIALIZACION EXTRACCION DE PALABRAS MEDIANTE PKE Y SPACY--------------"],"metadata":{"id":"36pwbKKKlMGQ","executionInfo":{"status":"ok","timestamp":1657157159982,"user_tz":300,"elapsed":51,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"36pwbKKKlMGQ","execution_count":19,"outputs":[]},{"cell_type":"code","source":["def extraccion_pke(document_txt, list_crudo):\n","  pos = {'NOUN', 'PROPN', 'ADJ', 'ADV'}\n","\n","  #Remover caracteres especiales de los textos y convertir el texto en lowercase\n","  texto_prueba = document_txt.strip().lower()\n","  #Variable para limpiar el texto de caracteres especiales \n","  CLEANR = re.compile('<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});') \n","  texto_prueba = re.sub(CLEANR, '', texto_prueba)\n","  #Quitar oraciones que se tengan entre brackets con la expresion regular \\{.*?\\}\n","  texto_prueba = re.sub(\"\\{.*?\\}\", \"\", texto_prueba) \n","  # Tokenizacion de las palabras y eliminación de las palabras claves previamente establecidas.\n","  texto_prueba_token = word_tokenize(texto_prueba)\n","  texto_prueba_sin_sw = [word for word in texto_prueba_token if not word in all_stopwords_gensim]\n","  filtered_text = \" \".join(texto_prueba_sin_sw)\n","  filtered_text\n","\n","  #Detalle PKE\n","  extractor = pke.unsupervised.TopicRank()\n","  extractor.load_document(input=filtered_text, language='en') # loading a document populates the extractor.sentences list (DETALLE PKE)\n","  extractor.candidate_selection(pos=pos)\n","  extractor.candidate_weighting()\n","  keyphrases = extractor.get_n_best(n=4, stemming=False)\n","  \n","  lista_keyword = [] #Lista de palabras claves extraidas con PKE\n","  \n","  for word,number in keyphrases:\n","      lista_keyword.append(word) #Creacion lista de keywords\n","      #lista_keyword + word + \",\"\n","      \n","  list_crudo.append((filtered_text,(lista_keyword)))\n","\n","  return list_crudo"],"metadata":{"id":"k0pFT-UMl38s","executionInfo":{"status":"ok","timestamp":1657157159984,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"k0pFT-UMl38s","execution_count":20,"outputs":[]},{"cell_type":"code","source":["def main_model(df_model):\n","  \n","    list_crudo = []\n","    \n","\n","    for k in df_model[\"id_txt\"]:\n","        file = open('/content/drive/MyDrive/Colab Notebooks/file_txt/'+k, \"r\")\n","        document = file.read()\n","        extraccion_pke(document, list_crudo)\n","        file.close()\n","    \n","    return list_crudo\n","\n","def create_df1(list_crudo):\n","  df1 = pd.DataFrame(list_crudo) #Transformar la lista del output en un dataframe\n","  df1 = df1.rename(columns={0:'file_text', 1: 'palabras_clave_pke'}) #renombrar columnas\n","\n","  return df1\n","\n","\n"],"metadata":{"id":"P7_BhQjGogGT","executionInfo":{"status":"ok","timestamp":1657157159985,"user_tz":300,"elapsed":53,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"P7_BhQjGogGT","execution_count":21,"outputs":[]},{"cell_type":"code","source":["output_principal = [] # Output principal donde se tiene URL, palabras claves con PKE y listas de 5 principales match con la similitud de spacy\n","\n","# count = 0 --> id_text\n","\n","def similitud(df1, df_ods,df_main):\n","\n","    for element,count in zip(df1.palabras_clave_pke,df_main.id_txt):\n","       \n","        lista_similarity_ods = [] #Lista donde se encuentran los 5 mayores resultados cuando se realizo el match de las keywords con el diccionario de ODS\n","        palabra_clave = element #Keyword de PKE\n","\n","        for word in  palabra_clave:\n","            for row, row2 in zip(df_ods['ODS'],df_ods['Keyword']):\n","                ODS = row #Numero de la ODS\n","                ODS_kw = row2 # Palabra relacionada a la ODS\n","                doc3 = nlp(word) #Vectorizacion de la keyword de PKE\n","                doc4 = nlp(ODS_kw) # Vectorizacion de la palabra relacionada de la ODS\n","                similarity = round(doc3.similarity(doc4),5) \n","                #lista_similarity_ods.append((word, ODS_kw, ODS, similarity)) #Lista con: keyword PKE, palabra clave PKE, Numero de la ODS, resultado de la similitud con spacy\n","                output_principal.append((word, ODS_kw, ODS, similarity,count)) #Creacion de output principal\n","\n","    return output_principal\n","    "],"metadata":{"id":"jTzQprhuo_lS","executionInfo":{"status":"ok","timestamp":1657157159985,"user_tz":300,"elapsed":52,"user":{"displayName":"Nicolas Rey","userId":"15934493870084309147"}}},"id":"jTzQprhuo_lS","execution_count":22,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  # Read URL list from .csv and the file text content then concat both and denomined 'df_main'\n","  \n","  # Inicio de main regex\n","\n","  df_main = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/url_list.csv')\n","  \n","  df_total = get_urls(df_main)\n","\n","  df_regex = main_regex(df_total)\n","\n","  # Inicio de main model\n","  \n","  list_crudo_ = main_model(df_total)\n","  \n","  df_1 = create_df1(list_crudo_)\n","\n","  # carga de dataframe ODS\n","  df_ods = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ODS.csv', sep=',') \n","\n","  output = similitud(df_1, df_ods, df_total)\n","\n","  df2 = pd.DataFrame(output) #Transformar la lista del output en un dataframe\n","  df2 = df2.rename(columns={0: 'palabras_clave_pke',1: 'keyword',2:'ODS',3:'similarity',4:'id'}) #renombrar columnas\n","\n","  # Obtencion de maximo en similitudes\n","  ks = df2.groupby(['id'],sort=False)['similarity'].max()\n","  ks.reset_index()\n","\n","  result = pd.merge(df2, ks,how=\"inner\", on=[\"id\", \"similarity\"])\n","  result.drop_duplicates('id')\n","\n","  # Concatenacion de Dataframes Regex y Modelo\n","  df_regex['keyword'] = result['keyword']\n","  df_regex['ods'] = result['ODS']\n","  df_regex['similarity_percentage'] = result[\"similarity\"]*100\n","  df_regex.head(2)\n","\n","  # Exportacion de Dataframe final a .csv cargada en Drive\n","  \n","  url = '/content/drive/MyDrive/Colab Notebooks/data_loaded.csv'\n","  load_csv_drive = df_regex.to_csv(url, index=False)\n","\n","    # Exportacion de Dataframe final a sheet \n","  auth.authenticate_user()\n","  creds, _ = default()\n","  gc = gspread.authorize(creds)\n","\n","\n","  auth.authenticate_user()\n","  creds, _ = default()\n","  gc = gspread.authorize(creds)\n","\n","\n","  spreadsheet_key = '1OWfmUpYvOqg9f_qDP2StgHUsLwg0wOBkxpLjjaP6Yls'\n","  workbook = gc.open_by_key(spreadsheet_key)\n","\n","  workbook.values_update(\n","    'Hoja 1!A1',\n","    params={\n","        'valueInputOption': 'USER_ENTERED'\n","    },\n","    body={\n","        'values': [df_regex.columns.values.tolist()] + df_regex.values.tolist()\n","    }\n","  )"],"metadata":{"id":"G-K2MZjY0dc0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c88b30f5-c24a-4e7d-dbac-8c72b21a33af"},"id":"G-K2MZjY0dc0","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: FutureWarning: Inferring datetime64[ns] from data containing strings is deprecated and will be removed in a future version. To retain the old behavior explicitly pass Series(data, dtype={value.dtype})\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:18: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"script_final.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}